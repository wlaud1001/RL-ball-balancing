{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "env._max_episode_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, **kargs):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(**kargs)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNet, self).__init__()\n",
    "        hidden = 10\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, env.action_space.n)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, step: 10\n",
      "ep: 2, step: 11\n",
      "ep: 17, step: 11\n",
      "ep: 25, step: 11\n",
      "ep: 61, step: 18\n",
      "ep: 63, step: 25\n",
      "ep: 67, step: 27\n",
      "ep: 71, step: 51\n",
      "ep: 72, step: 176\n",
      "ep: 80, step: 223\n",
      "lr:0.001, ep:99, max: 223.0\n",
      "ep: 197, step: 492\n",
      "lr:0.001, ep:199, max: 492.0\n",
      "ep: 213, step: 500\n",
      "ep: 249, step: 500\n",
      "lr:0.001, ep:299, max: 500.0\n",
      "ep: 314, step: 500\n",
      "ep: 388, step: 500\n",
      "lr:0.001, ep:399, max: 500.0\n",
      "ep: 401, step: 500\n",
      "ep: 408, step: 500\n",
      "ep: 413, step: 500\n",
      "ep: 442, step: 500\n",
      "ep: 463, step: 500\n",
      "ep: 496, step: 500\n",
      "lr:0.001, ep:499, max: 500.0\n"
     ]
    }
   ],
   "source": [
    "num_ep = 500\n",
    "num_batch = 50\n",
    "num_train = 50\n",
    "\n",
    "# reward list\n",
    "rList = []\n",
    "dis = 0.9\n",
    "\n",
    "model = QNet()\n",
    "target_model = QNet()\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "model.train()\n",
    "target_model.eval()\n",
    "\n",
    "lr = 1e-3\n",
    "criterion = nn.MSELoss()\n",
    "optim = torch.optim.Adam([p for p in model.parameters()], lr=lr)\n",
    "memory = ReplayMemory(1000000)\n",
    "\n",
    "#     e = exploration_max\n",
    "max_step = 0\n",
    "for i in range(num_ep):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    rAll = 0\n",
    "    e = 1. / (i + 10)\n",
    "    while not done:\n",
    "        env.render()\n",
    "        out = target_model(torch.tensor(state).float())\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = torch.argmax(out).item()\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # 'Transition', ('state', 'action', 'next_state', 'reward', 'done')\n",
    "        t = {\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'next_state': new_state,\n",
    "            'reward': reward if not done else -100,\n",
    "            'done': done,\n",
    "        }\n",
    "        memory.push(**t)\n",
    "        state = new_state\n",
    "        rAll += reward\n",
    "        step += 1\n",
    "\n",
    "        if step >= 10000:\n",
    "            break\n",
    "            \n",
    "    if step >= max_step:\n",
    "        best_model = model\n",
    "        max_step = step\n",
    "        print(f\"ep: {i}, step: {step}\")\n",
    "\n",
    "    rList.append(rAll)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\"lr:{}, ep:{}, max: {}\".format(lr, i, np.max(rList)))\n",
    "\n",
    "    # update every 10 episodes\n",
    "    if i % 10 == 0:\n",
    "        if len(memory) < num_batch:\n",
    "            pass\n",
    "        else:\n",
    "            for i in range(num_train):\n",
    "                # sampling from replay memory\n",
    "                batches = memory.sample(num_batch)\n",
    "                \n",
    "                batch_state = torch.tensor([b.state for b in batches]).float()\n",
    "                batch_next_state = torch.tensor([b.next_state for b in batches]).float()\n",
    "                batch_action = torch.tensor([b.action for b in batches]).float()\n",
    "                batch_reward = torch.tensor([b.reward for b in batches]).float()\n",
    "                batch_done = torch.tensor([b.done for b in batches]).float()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    target = batch_reward + dis * torch.max(target_model(batch_next_state), axis=1)[0]\n",
    "                target[batch_done == 1] = batch_reward[batch_done == 1]\n",
    "\n",
    "                pred = model(batch_state)\n",
    "                pred = pred.gather(1, batch_action.long().reshape(-1, 1))\n",
    "                loss = criterion(pred.flatten(), target)\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                \n",
    "            target_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "200.0\n",
      "300.0\n",
      "400.0\n",
      "500.0\n",
      "600.0\n",
      "700.0\n",
      "800.0\n",
      "900.0\n",
      "1000.0\n",
      "1100.0\n",
      "1200.0\n",
      "1300.0\n",
      "1400.0\n",
      "1500.0\n",
      "1600.0\n",
      "1700.0\n",
      "1800.0\n",
      "1900.0\n",
      "2000.0\n",
      "2100.0\n",
      "2200.0\n",
      "2300.0\n",
      "2400.0\n",
      "2500.0\n",
      "2600.0\n",
      "2700.0\n",
      "2800.0\n",
      "2900.0\n",
      "3000.0\n",
      "3100.0\n",
      "3200.0\n",
      "3300.0\n",
      "3400.0\n",
      "3500.0\n",
      "3600.0\n",
      "3700.0\n",
      "3800.0\n",
      "3900.0\n",
      "4000.0\n",
      "4100.0\n",
      "4200.0\n",
      "4300.0\n",
      "4400.0\n",
      "4500.0\n",
      "4600.0\n",
      "4700.0\n",
      "4800.0\n",
      "4900.0\n",
      "5000.0\n",
      "5100.0\n",
      "5200.0\n",
      "5300.0\n",
      "5400.0\n",
      "5500.0\n",
      "5600.0\n",
      "5700.0\n",
      "5800.0\n",
      "5900.0\n",
      "6000.0\n",
      "6100.0\n",
      "6200.0\n",
      "6300.0\n",
      "6400.0\n",
      "6500.0\n",
      "6600.0\n",
      "6700.0\n",
      "6800.0\n",
      "6900.0\n",
      "7000.0\n",
      "7100.0\n",
      "7200.0\n",
      "7300.0\n",
      "7400.0\n",
      "7500.0\n",
      "7600.0\n",
      "7700.0\n",
      "7800.0\n",
      "7900.0\n",
      "8000.0\n",
      "8100.0\n",
      "8200.0\n",
      "8300.0\n",
      "8400.0\n",
      "8500.0\n",
      "8600.0\n",
      "8700.0\n",
      "8800.0\n",
      "8900.0\n",
      "9000.0\n",
      "9100.0\n",
      "9200.0\n",
      "9300.0\n",
      "9400.0\n",
      "9500.0\n",
      "9600.0\n",
      "9700.0\n",
      "9800.0\n",
      "9900.0\n",
      "10000.0\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "state = env.reset()\n",
    "reward_sum = 0\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    pred = best_model(torch.tensor(state).float())\n",
    "    action = torch.argmax(pred).item()\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    reward_sum += reward\n",
    "    state = new_state\n",
    "    \n",
    "    if reward_sum % 100 == 0:\n",
    "        print(reward_sum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
